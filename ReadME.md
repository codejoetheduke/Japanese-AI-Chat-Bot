# **Speech-to-Speech AI Pipeline (Whisper ASR + Ollama LLM + Edge TTS)**

![alt text](image.jpg)

A real-time, multilingual, **speech-to-speech AI assistant** that listens, understands, and responds with natural voice output â€” all running **locally**.

This project combines:

- **Automatic Speech Recognition (ASR)** using Whisper & specialized models
- **Local LLM reasoning** using Ollama
- **High-quality Text-to-Speech (TTS)** using Microsoft Edge TTS (free, no API key needed)

Supports **20+ languages**, including English, Japanese, Chinese, Spanish, French, Arabic, Korean, and more.

---

## ğŸš€ **Features**

### ğŸ¤ **1. Speech-to-Text (ASR)**

- Uses **OpenAI Whisper** and specialized models
- Japanese uses **kotoba-tech/kotoba-whisper-v1.0** for superior accuracy
- Supports automatic normalization, resampling, and long audio segments

### ğŸ¤– **2. LLM Reasoning (Ollama)**

- Integrates seamlessly with **Ollamaâ€™s local LLMs**
- Supports any model (Llama, Qwen, Mistral, Phi, etc.)
- Keeps **conversation history** for natural dialogue

### ğŸ”Š **3. Text-to-Speech (Edge TTS)**

- Uses **Microsoft Edge Neural Voices**
- Completely free, high-quality, and supports 90+ voices
- No API keys or cloud access required

### ğŸŒ **4. Multilingual Support**

Preconfigured for high-quality voices + models:

```
English, Japanese, Chinese, Spanish, French, German,
Italian, Portuguese, Korean, Russian, Arabic, Hindi,
Polish, Turkish, Dutch, Czech, Hungarian, Swedish,
Norwegian, Finnish
```

---

## ğŸ“¦ **Installation**

### 1. Install Python packages

```bash
pip install torch transformers sounddevice scipy numpy requests edge-tts
```

(Optional for MP3 â†’ WAV conversion and playback)

```bash
pip install pydub pygame
```

---

### 2. Install and run Ollama

Download from: [https://ollama.com](https://ollama.com)

Start server:

```bash
ollama serve
```

Pull the model you want to use (example):

```bash
ollama pull llama3.2:3b
```

---

## ğŸ”§ **Configuration**

Inside the script:

```python
LANGUAGE = "japanese"
OLLAMA_MODEL = "gpt-oss:120b-cloud"
CUSTOM_VOICE = None
```

You can switch:

- **LANGUAGE** â†’ any from `SUPPORTED_LANGUAGES`
- **OLLAMA_MODEL** â†’ any model installed in Ollama
- **CUSTOM_VOICE** â†’ any Edge TTS voice name (optional)

List voices for a language:

```python
SpeechPipelineEdgeTTS.print_available_voices(language_filter="ja")
```

---

## â–¶ï¸ **Usage**

### **Start the full pipeline**

```bash
python app.py
```

The program will:

1. Record 5 seconds of your speech
2. Transcribe it using Whisper
3. Send text to the Ollama LLM
4. Convert the reply to natural speech
5. Play the audio output

You can continue chatting in a loop.

---

## ğŸ“ **Project Structure**

```
â”œâ”€â”€ SpeechPipelineEdgeTTS
â”‚   â”œâ”€â”€ ASR (Whisper / Kotoba)
â”‚   â”œâ”€â”€ Ollama LLM Chat Interface
â”‚   â”œâ”€â”€ Edge TTS Voice Synthesis
â”‚   â”œâ”€â”€ Microphone + Audio Playback
â”‚   â”œâ”€â”€ Conversation Memory Handling
â””â”€â”€ README.md
```

---

## ğŸ§  **How It Works**

### 1. **Record Microphone Audio**

Uses `sounddevice` for high-quality capture.

### 2. **Transcribe (Speech â†’ Text)**

Runs a Whisper-based model optimized for the selected language.

### 3. **LLM Processing**

Sends text to Ollama with configurable temperature, memory, and model selection.

### 4. **Generate Natural Speech**

Converts the LLM output into speech using Edge TTS
(Saves MP3 â†’ Converts to WAV â†’ Plays audio)

---

## ğŸŒ **Supported Languages & Voices**

Each language maps to:

- Whisper language mode
- Specialized ASR model
- Best Edge TTS neural voice

You can customize these through the dictionary:

```python
SUPPORTED_LANGUAGES = {
    'japanese': {
        'whisper': 'japanese',
        'voice': 'ja-JP-NanamiNeural',
        'asr_model': 'kotoba-tech/kotoba-whisper-v1.0'
    }
}
```

---

## ğŸ”„ **Conversation Memory**

Each interaction is stored:

```python
self.conversation_history.append({"role": "user", "content": text})
self.conversation_history.append({"role": "assistant", "content": ai_response})
```

Auto-clears old messages to avoid memory bloating.

Reset manually:

```python
pipeline.reset_conversation()
```

---

## ğŸ—£ **Custom Voices**

Use any Edge TTS voice:

```python
CUSTOM_VOICE = "ja-JP-KeitaNeural"
```

Find all voices:

```python
SpeechPipelineEdgeTTS.print_available_voices()
```

---

## ğŸ›  Troubleshooting

### â— Ollama not detected

Make sure it's running:

```bash
ollama serve
```

### â— MP3/WAV playback not working

Install the fallback:

```bash
pip install pygame
```

### â— Whisper too slow

Switch to a smaller ASR model:

```python
'asr_model': 'openai/whisper-small'
```

---

## â­ **Future Improvements**

- Streaming ASR + Streaming TTS
- Realtime echo cancellation
- Web UI (Gradio / FastAPI)
- Hotword activation (â€œHey Assistantâ€¦â€)

---

## ğŸ“œ **License**

MIT License

---

## ğŸ‘¨â€ğŸ’» Author

**Duke Kojo Kongo** (_CodeJoe_)  
Data Scientist â€¢ AI Engineer â€¢ Builder of Intelligent Systems

---
